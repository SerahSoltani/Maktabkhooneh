{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"Vo2jLoeMd-oO"},"outputs":[{"name":"stdout","output_type":"stream","text":["Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n","170498071/170498071 [==============================] - 3s 0us/step\n","Epoch 1/20\n","704/704 [==============================] - 108s 149ms/step - loss: 1.6124 - accuracy: 0.4292 - val_loss: 1.3024 - val_accuracy: 0.5290\n","Epoch 2/20\n","704/704 [==============================] - 94s 133ms/step - loss: 1.2562 - accuracy: 0.5580 - val_loss: 1.1338 - val_accuracy: 0.6048\n","Epoch 3/20\n","704/704 [==============================] - 94s 134ms/step - loss: 1.1331 - accuracy: 0.6027 - val_loss: 1.0936 - val_accuracy: 0.6168\n","Epoch 4/20\n","704/704 [==============================] - 96s 136ms/step - loss: 1.0538 - accuracy: 0.6337 - val_loss: 1.0066 - val_accuracy: 0.6540\n","Epoch 5/20\n","704/704 [==============================] - 95s 134ms/step - loss: 0.9824 - accuracy: 0.6595 - val_loss: 0.9971 - val_accuracy: 0.6578\n","Epoch 6/20\n","704/704 [==============================] - 96s 137ms/step - loss: 0.9227 - accuracy: 0.6781 - val_loss: 0.9729 - val_accuracy: 0.6674\n","Epoch 7/20\n","704/704 [==============================] - 95s 135ms/step - loss: 0.8706 - accuracy: 0.6981 - val_loss: 0.9573 - val_accuracy: 0.6764\n","Epoch 8/20\n","704/704 [==============================] - 95s 135ms/step - loss: 0.8253 - accuracy: 0.7144 - val_loss: 0.9040 - val_accuracy: 0.6954\n","Epoch 9/20\n","704/704 [==============================] - 95s 135ms/step - loss: 0.7824 - accuracy: 0.7286 - val_loss: 0.9294 - val_accuracy: 0.6826\n","Epoch 10/20\n","704/704 [==============================] - 95s 135ms/step - loss: 0.7404 - accuracy: 0.7420 - val_loss: 0.9520 - val_accuracy: 0.6828\n","Epoch 11/20\n","704/704 [==============================] - 97s 138ms/step - loss: 0.7165 - accuracy: 0.7505 - val_loss: 0.9317 - val_accuracy: 0.6866\n","Epoch 12/20\n","704/704 [==============================] - 115s 163ms/step - loss: 0.6766 - accuracy: 0.7645 - val_loss: 0.9286 - val_accuracy: 0.6992\n","Epoch 13/20\n","704/704 [==============================] - 96s 136ms/step - loss: 0.6375 - accuracy: 0.7754 - val_loss: 0.9762 - val_accuracy: 0.6886\n","Epoch 14/20\n","700/704 [============================\u003e.] - ETA: 0s - loss: 0.6115 - accuracy: 0.7857"]}],"source":["import numpy as np\n","import tensorflow as tf\n","from tensorflow.keras.datasets import cifar10\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, Flatten, Dropout\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.initializers import lecun_normal\n","import matplotlib.pyplot as plt\n","\n","# Load and preprocess the CIFAR-10 dataset\n","(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n","x_train, x_val = x_train[:45000], x_train[45000:]\n","y_train, y_val = y_train[:45000], y_train[45000:]\n","\n","x_train = x_train.astype('float32') / 255.0\n","x_val = x_val.astype('float32') / 255.0\n","x_test = x_test.astype('float32') / 255.0\n","\n","y_train = tf.keras.utils.to_categorical(y_train, 10)\n","y_val = tf.keras.utils.to_categorical(y_val, 10)\n","y_test = tf.keras.utils.to_categorical(y_test, 10)\n","\n","# Function to create the model\n","def create_model(conv_1_units=64, conv_2_units=64, conv_3_units=64, dense_units=256, learning_rate=0.001):\n","    model = Sequential([\n","        Conv2D(conv_1_units, (3, 3), activation='selu', kernel_initializer='lecun_normal', input_shape=(32, 32, 3)),\n","        MaxPooling2D((2, 2)),\n","        Conv2D(conv_2_units, (3, 3), activation='selu', kernel_initializer='lecun_normal'),\n","        MaxPooling2D((2, 2)),\n","        Conv2D(conv_3_units, (3, 3), activation='selu', kernel_initializer='lecun_normal'),\n","        MaxPooling2D((2, 2)),\n","        Flatten(),\n","        Dense(dense_units, activation='selu', kernel_initializer='lecun_normal'),\n","        Dropout(0.5),\n","        Dense(10, activation='softmax')\n","    ])\n","\n","    optimizer = Adam(learning_rate=learning_rate)\n","    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n","\n","    return model\n","\n","# Create the model\n","model = create_model()\n","\n","# Train the model\n","history = model.fit(x_train, y_train, epochs=20, validation_data=(x_val, y_val), batch_size=64)\n","\n","# Plot accuracy and loss\n","plt.figure(figsize=(14, 5))\n","\n","plt.subplot(1, 2, 1)\n","plt.plot(history.history['accuracy'], label='Train Accuracy')\n","plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n","plt.title('Model Accuracy')\n","plt.xlabel('Epoch')\n","plt.ylabel('Accuracy')\n","plt.legend()\n","\n","plt.subplot(1, 2, 2)\n","plt.plot(history.history['loss'], label='Train Loss')\n","plt.plot(history.history['val_loss'], label='Validation Loss')\n","plt.title('Model Loss')\n","plt.xlabel('Epoch')\n","plt.ylabel('Loss')\n","plt.legend()\n","\n","plt.show()\n"]},{"cell_type":"markdown","metadata":{"id":"SykAU2DLizhy"},"source":["activation مرحله قبل را عوض کنید و بعد از هر لایه به جز لایه آخر از BatchNormalization استفاده کنید. نتایج این مرحله را در کنار مرحله قبل در یک گراف رسم کرده و مقایسه کنید."]},{"cell_type":"markdown","metadata":{"id":"jkqq6cQWirDD"},"source":["در این کد:\n","\n","دو مدل ساخته می‌شوند: یکی با activation SELU و دیگری با activation ReLU و استفاده از Batch Normalization.\n","هر دو مدل با داده‌های آموزشی و اعتبارسنجی آموزش داده می‌شوند.\n","دقت و خطا برای هر دو مدل رسم شده و مقایسه می‌شوند."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"X7zZsEPNiiD7"},"outputs":[],"source":["import numpy as np\n","import tensorflow as tf\n","from tensorflow.keras.datasets import cifar10\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, Flatten, Dropout, BatchNormalization\n","from tensorflow.keras.optimizers import Adam\n","import matplotlib.pyplot as plt\n","\n","# Load and preprocess the CIFAR-10 dataset\n","(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n","x_train, x_val = x_train[:45000], x_train[45000:]\n","y_train, y_val = y_train[:45000], y_train[45000:]\n","\n","x_train = x_train.astype('float32') / 255.0\n","x_val = x_val.astype('float32') / 255.0\n","x_test = x_test.astype('float32') / 255.0\n","\n","y_train = tf.keras.utils.to_categorical(y_train, 10)\n","y_val = tf.keras.utils.to_categorical(y_val, 10)\n","y_test = tf.keras.utils.to_categorical(y_test, 10)\n","\n","# Function to create the model with SELU activation and Batch Normalization\n","def create_model_with_selu(conv_1_units=64, conv_2_units=64, conv_3_units=64, dense_units=256, learning_rate=0.001):\n","    model = Sequential([\n","        Conv2D(conv_1_units, (3, 3), activation='selu', kernel_initializer='lecun_normal', input_shape=(32, 32, 3)),\n","        MaxPooling2D((2, 2)),\n","        Conv2D(conv_2_units, (3, 3), activation='selu', kernel_initializer='lecun_normal'),\n","        MaxPooling2D((2, 2)),\n","        Conv2D(conv_3_units, (3, 3), activation='selu', kernel_initializer='lecun_normal'),\n","        MaxPooling2D((2, 2)),\n","        Flatten(),\n","        Dense(dense_units, activation='selu', kernel_initializer='lecun_normal'),\n","        Dropout(0.5),\n","        Dense(10, activation='softmax')\n","    ])\n","\n","    optimizer = Adam(learning_rate=learning_rate)\n","    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n","\n","    return model\n","\n","# Function to create the model with ReLU activation and Batch Normalization\n","def create_model_with_relu_and_bn(conv_1_units=64, conv_2_units=64, conv_3_units=64, dense_units=256, learning_rate=0.001):\n","    model = Sequential([\n","        Conv2D(conv_1_units, (3, 3), activation='relu', kernel_initializer='he_normal', input_shape=(32, 32, 3)),\n","        BatchNormalization(),\n","        MaxPooling2D((2, 2)),\n","        Conv2D(conv_2_units, (3, 3), activation='relu', kernel_initializer='he_normal'),\n","        BatchNormalization(),\n","        MaxPooling2D((2, 2)),\n","        Conv2D(conv_3_units, (3, 3), activation='relu', kernel_initializer='he_normal'),\n","        BatchNormalization(),\n","        MaxPooling2D((2, 2)),\n","        Flatten(),\n","        Dense(dense_units, activation='relu', kernel_initializer='he_normal'),\n","        BatchNormalization(),\n","        Dropout(0.5),\n","        Dense(10, activation='softmax')\n","    ])\n","\n","    optimizer = Adam(learning_rate=learning_rate)\n","    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n","\n","    return model\n","\n","# Train the SELU model\n","selu_model = create_model_with_selu()\n","selu_history = selu_model.fit(x_train, y_train, epochs=20, validation_data=(x_val, y_val), batch_size=64)\n","\n","# Train the ReLU + Batch Normalization model\n","relu_bn_model = create_model_with_relu_and_bn()\n","relu_bn_history = relu_bn_model.fit(x_train, y_train, epochs=20, validation_data=(x_val, y_val), batch_size=64)\n","\n","# Plot accuracy and loss for both models\n","plt.figure(figsize=(14, 10))\n","\n","plt.subplot(2, 2, 1)\n","plt.plot(selu_history.history['accuracy'], label='Train Accuracy (SELU)')\n","plt.plot(selu_history.history['val_accuracy'], label='Validation Accuracy (SELU)')\n","plt.plot(relu_bn_history.history['accuracy'], label='Train Accuracy (ReLU+BN)')\n","plt.plot(relu_bn_history.history['val_accuracy'], label='Validation Accuracy (ReLU+BN)')\n","plt.title('Model Accuracy')\n","plt.xlabel('Epoch')\n","plt.ylabel('Accuracy')\n","plt.legend()\n","\n","plt.subplot(2, 2, 2)\n","plt.plot(selu_history.history['loss'], label='Train Loss (SELU)')\n","plt.plot(selu_history.history['val_loss'], label='Validation Loss (SELU)')\n","plt.plot(relu_bn_history.history['loss'], label='Train Loss (ReLU+BN)')\n","plt.plot(relu_bn_history.history['val_loss'], label='Validation Loss (ReLU+BN)')\n","plt.title('Model Loss')\n","plt.xlabel('Epoch')\n","plt.ylabel('Loss')\n","plt.legend()\n","\n","plt.show()\n"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyPEWnCceCQxqJKPog1eL86Q","name":"","version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}