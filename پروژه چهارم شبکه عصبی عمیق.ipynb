{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOJvhns8C0w7ELlcQ8HJoe5"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["برای اضافه کردن بهینه‌ساز RMSprop with Nesterov به کلاس Optimizers و اجرای 20 iteration از این بهینه‌ساز روی یک تابع دو بعدی، می‌توانیم از کد زیر استفاده کنیم. ابتدا کد اصلی کلاس Optimizers را داریم و سپس بهینه‌ساز جدید را اضافه می‌کنیم."],"metadata":{"id":"OuuHDkEBPE3z"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wLaNApXwOpe6","executionInfo":{"status":"ok","timestamp":1720591388214,"user_tz":-180,"elapsed":834,"user":{"displayName":"Sara Soltani","userId":"00129795583987472913"}},"outputId":"79c5acd5-5dce-4c97-c081-9ad143ea6ea2"},"outputs":[{"output_type":"stream","name":"stdout","text":["Iteration 1: x = [ 1.96837722 -2.96837722], f(x) = 12.685772235956808\n","Iteration 2: x = [ 1.91730949 -2.91719627], f(x) = 12.186109763066131\n","Iteration 3: x = [ 1.85275449 -2.85232892], f(x) = 11.568479473402038\n","Iteration 4: x = [ 1.77849728 -2.77748113], f(x) = 10.877453989815292\n","Iteration 5: x = [ 1.69721654 -2.6952563 ], f(x) = 10.144950547843543\n","Iteration 6: x = [ 1.61092001 -2.60758951], f(x) = 9.394586349451918\n","Iteration 7: x = [ 1.52116616 -2.51596831], f(x) = 8.644043005088035\n","Iteration 8: x = [ 1.42919358 -2.42156127], f(x) = 7.906553286204526\n","Iteration 9: x = [ 1.33600376 -2.32529994], f(x) = 7.191925846003258\n","Iteration 10: x = [ 1.24241755 -2.22793452], f(x) = 6.507293621480802\n","Iteration 11: x = [ 1.14911578 -2.13007367], f(x) = 5.857680911453584\n","Iteration 12: x = [ 1.05666936 -2.03221385], f(x) = 5.246443257042195\n","Iteration 13: x = [ 0.96556244 -1.93476173], f(x) = 4.675613785260718\n","Iteration 14: x = [ 0.87621045 -1.83805161], f(x) = 4.146178476583546\n","Iteration 15: x = [ 0.78897455 -1.74235914], f(x) = 3.6582962032557385\n","Iteration 16: x = [ 0.7041732  -1.64791241], f(x) = 3.2114752155867614\n","Iteration 17: x = [ 0.6220916  -1.55490097], f(x) = 2.8047149770504363\n","Iteration 18: x = [ 0.54298931 -1.46348315], f(x) = 2.4366203163161764\n","Iteration 19: x = [ 0.46710636 -1.37379224], f(x) = 2.1054934714215547\n","Iteration 20: x = [ 0.39466801 -1.28594157], f(x) = 1.80940857012757\n"]},{"output_type":"execute_result","data":{"text/plain":["array([ 0.39466801, -1.28594157])"]},"metadata":{},"execution_count":1}],"source":["import numpy as np\n","\n","class Optimizers:\n","    def __init__(self, func, grad_func, lr=0.01, beta=0.9, epsilon=1e-8, gamma=0.9):\n","        self.func = func\n","        self.grad_func = grad_func\n","        self.lr = lr\n","        self.beta = beta\n","        self.epsilon = epsilon\n","        self.gamma = gamma\n","\n","    def rmsprop_with_nesterov(self, x_init, iterations=20):\n","        x = np.array(x_init, dtype=np.float64)\n","        v = np.zeros_like(x)\n","        s = np.zeros_like(x)\n","        for i in range(iterations):\n","            g = self.grad_func(x - self.gamma * v)\n","            s = self.beta * s + (1 - self.beta) * g**2\n","            v = self.gamma * v + self.lr * g / (np.sqrt(s) + self.epsilon)\n","            x -= v\n","            print(f\"Iteration {i+1}: x = {x}, f(x) = {self.func(x)}\")\n","        return x\n","\n","# مثال استفاده از کلاس\n","def func(x):\n","    return np.sum(x**2)\n","\n","def grad_func(x):\n","    return 2*x\n","\n","# مقداردهی اولیه\n","x_init = [2, -3]\n","\n","# ایجاد شیء Optimizers\n","optimizer = Optimizers(func, grad_func, lr=0.01, beta=0.9, epsilon=1e-8, gamma=0.9)\n","\n","# اجرای بهینه‌ساز RMSprop with Nesterov\n","optimizer.rmsprop_with_nesterov(x_init, iterations=20)\n"]},{"cell_type":"markdown","source":["این کد شامل موارد زیر است:\n","\n","تعریف یک کلاس به نام Optimizers که شامل پارامترهای مورد نیاز برای بهینه‌سازی است.\n","اضافه کردن یک تابع rmsprop_with_nesterov به کلاس که بهینه‌ساز RMSprop with Nesterov را پیاده‌سازی می‌کند.\n","تعریف یک تابع نمونه func و گرادیان آن grad_func برای تست بهینه‌ساز.\n","مقداردهی اولیه نقطه شروع بهینه‌سازی و اجرای بهینه‌ساز برای 20 iteration.\n","با اجرای این کد، مقدارهای بهینه شده و مقدار تابع در هر iteration چاپ می‌شوند."],"metadata":{"id":"WjBpTQtBPJgp"}}]}